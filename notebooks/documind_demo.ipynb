{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DocuMind - AI Document Intelligence Agent\n",
        "\n",
        "This notebook demonstrates the capabilities of DocuMind, a multi-agent system for automated document analysis.\n",
        "\n",
        "## Features Demonstrated:\n",
        "1. Document Reading (PDF, Text, URL)\n",
        "2. Information Extraction (Tables, Metrics, Dates, Tasks, Entities)\n",
        "3. Multiple Summary Types (Executive, Bullet, TL;DR)\n",
        "4. Question Answering with Citations\n",
        "5. Memory Storage\n",
        "6. Quality Evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q PyPDF2 pdfplumber beautifulsoup4 requests openai langchain sentence-transformers chromadb rouge-score nltk spacy dateparser\n",
        "!python -m spacy download en_core_web_sm -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# Add documind to path\n",
        "sys.path.append('../')\n",
        "\n",
        "# Set your OpenAI API key\n",
        "os.environ['OPENAI_API_KEY'] = 'your-api-key-here'  # Replace with your key\n",
        "\n",
        "from documind import DocuMind\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize DocuMind\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize DocuMind\n",
        "dm = DocuMind(\n",
        "    api_key=os.getenv('OPENAI_API_KEY'),\n",
        "    ocr_enabled=True,\n",
        "    memory_enabled=True,\n",
        "    evaluation_enabled=True\n",
        ")\n",
        "\n",
        "print(\"DocuMind initialized successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 1: Process a PDF Document\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process a PDF document\n",
        "# Replace with path to your PDF\n",
        "pdf_path = \"sample_document.pdf\"\n",
        "\n",
        "result = dm.process_document(\n",
        "    source=pdf_path,\n",
        "    tasks=[\"extract\", \"summarize\", \"evaluate\"],\n",
        "    store_in_memory=True\n",
        ")\n",
        "\n",
        "print(f\"Document ID: {result['document_id']}\")\n",
        "print(f\"Total pages: {result['document']['metadata'].get('total_pages', 'N/A')}\")\n",
        "print(f\"Total words: {result['document']['metadata'].get('total_words', 'N/A')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 2: View Extractions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View extracted information\n",
        "extractions = result['extractions']\n",
        "\n",
        "print(f\"Tables found: {len(extractions.get('tables', []))}\")\n",
        "print(f\"Metrics found: {len(extractions.get('metrics', []))}\")\n",
        "print(f\"Dates found: {len(extractions.get('dates', []))}\")\n",
        "print(f\"Tasks found: {len(extractions.get('tasks', []))}\")\n",
        "\n",
        "# Display sample metrics\n",
        "if extractions.get('metrics'):\n",
        "    print(\"\\nSample Metrics:\")\n",
        "    for metric in extractions['metrics'][:5]:\n",
        "        print(f\"  - {metric['value']} ({metric['type']})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 3: View Summaries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display summaries\n",
        "summaries = result['summaries']\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"EXECUTIVE SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "print(summaries.get('executive', 'N/A'))\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"BULLET-POINT SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "print(summaries.get('bullet', 'N/A'))\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"TL;DR SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "print(summaries.get('tldr', 'N/A'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 4: Question Answering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up Q&A\n",
        "result = dm.process_document(\n",
        "    source=pdf_path,\n",
        "    tasks=[\"qa\"]\n",
        ")\n",
        "\n",
        "# Ask questions\n",
        "questions = [\n",
        "    \"What is the main objective of this document?\",\n",
        "    \"What are the key findings?\",\n",
        "    \"What are the recommendations?\"\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "    answer = dm.answer_question(question, return_citations=True)\n",
        "    print(f\"\\nQ: {question}\")\n",
        "    print(f\"A: {answer['answer']}\")\n",
        "    print(f\"Confidence: {answer['confidence']:.2f}\")\n",
        "    if answer.get('citations'):\n",
        "        print(f\"Citations: Pages {[c['page'] for c in answer['citations']]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 5: Evaluation Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View evaluation results\n",
        "evaluations = result.get('evaluations', {})\n",
        "\n",
        "if 'summary_executive' in evaluations:\n",
        "    eval_result = evaluations['summary_executive']\n",
        "    print(\"Executive Summary Evaluation:\")\n",
        "    print(f\"  Overall Score: {eval_result['overall_score']:.2f}\")\n",
        "    print(f\"  Clarity: {eval_result['clarity']:.2f}\")\n",
        "    print(f\"  Completeness: {eval_result['completeness']:.2f}\")\n",
        "    \n",
        "    if eval_result.get('suggestions'):\n",
        "        print(\"  Suggestions:\")\n",
        "        for suggestion in eval_result['suggestions']:\n",
        "            print(f\"    - {suggestion}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "DocuMind provides a comprehensive solution for document analysis with:\n",
        "- Multi-format support (PDF, text, URLs)\n",
        "- Intelligent extraction\n",
        "- Multiple summary types\n",
        "- Q&A with citations\n",
        "- Memory persistence\n",
        "- Quality evaluation\n",
        "\n",
        "For more information, see the README.md file.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
